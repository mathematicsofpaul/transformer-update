An updated version of the Harvard NLP Annotated Transformer. This code is not orginally mine and all that was done by me was the necessary bugs fixes to run it on Pytorch 1.5.1 with GPUs enabled. In this case 2 gpus, were used to train the model. I also removed the last section on the attention visualizations and translation task, since my goal was to repurpose this code for something else. 


Original Code for The Annotated Transformer blog post:

http://nlp.seas.harvard.edu/2018/04/03/attention.html



