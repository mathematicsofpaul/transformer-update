An updated version of the Harvard NLP Annotated Transformer. This code is not orginally mine and all that was done by me was the necessary bugs fixes to run it on Pytorch 1.5.1 with GPUs enabled. In this case 2 gpus, were used to train the model.  


Original Code for The Annotated Transformer blog post:

http://nlp.seas.harvard.edu/2018/04/03/attention.html
